{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Collecting scikit-learn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/57/8a9889d49d0d77905af5a7524fb2b468d2ef5fc723684f51f5ca63efed0d/scikit_learn-0.21.3-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (10.5MB)\n",
      "\u001b[K     |████████████████████████████████| 10.5MB 2.2MB/s eta 0:00:01    |██▋                             | 870kB 2.2MB/s eta 0:00:05     |██████                          | 1.9MB 2.2MB/s eta 0:00:04     |█████████                       | 2.9MB 2.2MB/s eta 0:00:04\n",
      "\u001b[?25hCollecting numpy>=1.11.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/9a/a6b3168f2194fb468dcc4cf54c8344d1f514935006c3347ede198e968cb0/numpy-1.17.4-cp37-cp37m-macosx_10_9_x86_64.whl (15.1MB)\n",
      "\u001b[K     |████████████████████████████████| 15.1MB 24.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=0.11\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/42/155696f85f344c066e17af287359c9786b436b1bf86029bb3411283274f3/joblib-0.14.0-py2.py3-none-any.whl (294kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 14.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=0.17.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/40/c7d54f604f41656be3eb8b8bd7d073050e3da08b5b0d32bba8c8c7023210/scipy-1.3.2-cp37-cp37m-macosx_10_6_intel.whl (27.7MB)\n",
      "\u001b[K     |████████████████████████████████| 27.7MB 5.3MB/s eta 0:00:01    |██▏                             | 1.8MB 7.6MB/s eta 0:00:04\n",
      "\u001b[?25hBuilding wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=b5c14ed04c4643c1b5197db185ef1dae974772fb3526bc01e74334659f0eba82\n",
      "  Stored in directory: /Users/jbiller/Library/Caches/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "Installing collected packages: numpy, joblib, scipy, scikit-learn, sklearn\n",
      "Successfully installed joblib-0.14.0 numpy-1.17.4 scikit-learn-0.21.3 scipy-1.3.2 sklearn-0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting panda\n",
      "  Downloading https://files.pythonhosted.org/packages/79/03/74996420528fe488ce17c42b6400531c8067d7eb661c304fa3aa8fdad17c/panda-0.3.1.tar.gz\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from panda) (41.6.0)\n",
      "Collecting requests\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 2.4MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/40/a9837291310ee1ccc242ceb6ebfd9eb21539649f193a7c8c86ba15b98539/urllib3-1.25.7-py2.py3-none-any.whl (125kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 5.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/b0/8146a4f8dd402f60744fa380bc73ca47303cccf8b9190fd16a827281eac2/certifi-2019.9.11-py2.py3-none-any.whl (154kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 28.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<2.9,>=2.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 9.5MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting chardet<3.1.0,>=3.0.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 26.5MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: panda\n",
      "  Building wheel for panda (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for panda: filename=panda-0.3.1-cp37-none-any.whl size=7259 sha256=47f0d8a8640689a45d1e294fc955c4bdfcb520a7be5aa62d25068001732b3d5d\n",
      "  Stored in directory: /Users/jbiller/Library/Caches/pip/wheels/c6/c8/45/06ed898b0bb401c1ff207dbb05b1587ff28860a236d98b1996\n",
      "Successfully built panda\n",
      "Installing collected packages: urllib3, certifi, idna, chardet, requests, panda\n",
      "Successfully installed certifi-2019.9.11 chardet-3.0.4 idna-2.8 panda-0.3.1 requests-2.22.0 urllib3-1.25.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install panda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fancyimpute\n",
      "  Downloading https://files.pythonhosted.org/packages/8e/da/418fb3f488c7fd79436fc0736fb9641819c5bb3d090d26a14ac68dc5097b/fancyimpute-0.5.4.tar.gz\n",
      "Collecting knnimpute\n",
      "  Downloading https://files.pythonhosted.org/packages/e8/35/e9ab006430d36aed71f623db8d194a723a14d40b52b3405cdd5b94d9de04/knnimpute-0.1.0.tar.gz\n",
      "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.7/site-packages (from fancyimpute) (1.17.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/site-packages (from fancyimpute) (1.3.2)\n",
      "Collecting cvxpy>=1.0.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/6e/628710ad996da232814bbcfce9984e1bb12683504e9fff7e0fe9ebc97785/cvxpy-1.0.25-cp37-cp37m-macosx_10_9_x86_64.whl (676kB)\n",
      "\u001b[K     |████████████████████████████████| 686kB 3.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.2 in /usr/local/lib/python3.7/site-packages (from fancyimpute) (0.21.3)\n",
      "Collecting keras>=2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
      "\u001b[K     |████████████████████████████████| 378kB 20.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/72/6b3264aa2889b7dde7663464b99587d95cd6a5f3b9b30181f14d78a63e64/tensorflow-2.0.0-cp37-cp37m-macosx_10_11_x86_64.whl (102.7MB)\n",
      "\u001b[K     |████████████████████████████████| 102.7MB 33.5MB/s eta 0:00:01  |▋                               | 2.0MB 3.9MB/s eta 0:00:26     |███████                         | 22.7MB 31.9MB/s eta 0:00:03     |████████▌                       | 27.3MB 31.9MB/s eta 0:00:03     |████████████████████▋           | 66.3MB 26.9MB/s eta 0:00:02     |█████████████████████▋          | 69.4MB 26.9MB/s eta 0:00:02     |██████████████████████▉         | 73.4MB 26.9MB/s eta 0:00:02     |█████████████████████████       | 80.1MB 26.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/Cellar/ipython/7.8.0/libexec/vendor/lib/python3.7/site-packages (from knnimpute->fancyimpute) (1.12.0)\n",
      "Collecting multiprocess\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/17/5151b6ac2ac9b6276d46c33369ff814b0901872b2a0871771252f02e9192/multiprocess-0.70.9.tar.gz (1.6MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6MB 17.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scs>=1.1.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/6e/dbdd778c64c1920ae357a2013ea655d65a1f8b60f397d6e5549e4aafe8ec/scs-2.1.1-2.tar.gz (157kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 13.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ecos>=2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/3a/59aa93b573a22fda44402383aeddcc2a081c31e61080af3da9d11855c77a/ecos-2.0.7.post1.tar.gz (126kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 26.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting osqp>=0.4.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/b7/1c70a2371de41d0a0d517ec8c060e741c05c30f21a9cc5a8abc239fdd302/osqp-0.6.1-cp37-cp37m-macosx_10_9_x86_64.whl (158kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 14.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/site-packages (from scikit-learn>=0.21.2->fancyimpute) (0.14.0)\n",
      "Collecting h5py\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/8b/4d01ae9a9d50a0bcc7b0b9aae41785d8d9de6fa9bba04dc20b1582181d2d/h5py-2.10.0-cp37-cp37m-macosx_10_6_intel.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 7.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-applications>=1.0.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 10.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.0.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 12.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/e8/b3212641ee2718d556df0f23f78de8303f068fe29cdaa7a91018849582fe/PyYAML-5.1.2.tar.gz (265kB)\n",
      "\u001b[K     |████████████████████████████████| 266kB 26.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astor>=0.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Collecting wrapt>=1.11.1\n",
      "  Downloading https://files.pythonhosted.org/packages/23/84/323c2415280bc4fc880ac5050dddfb3c8062c2552b34c2e512eb4aa68f79/wrapt-1.11.2.tar.gz\n",
      "Collecting tensorboard<2.1.0,>=2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/9e/a48cd34dd7b672ffc227b566f7d16d63c62c58b542d54efa45848c395dd4/tensorboard-2.0.1-py3-none-any.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 22.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.8.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/27/0413a5dffd7ddca4ea43cffd22f46ec2b26a5ed18c974e4448763e758a9b/grpcio-1.25.0-cp37-cp37m-macosx_10_9_x86_64.whl (2.3MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3MB 14.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.7.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/72/e6e483e2db953c11efa44ee21c5fdb6505c4dffa447b4263ca8af6676b62/absl-py-0.8.1.tar.gz (103kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 10.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.2.2\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
      "\u001b[K     |████████████████████████████████| 450kB 26.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.6.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/c6/a8b6a74ab1e165f0aaa673a46f5c895af8780976880c98934ae82060356d/protobuf-3.10.0-cp37-cp37m-macosx_10_9_intel.whl (1.4MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 25.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/83/755bd5324777875e9dff19c2e59daec837d0378c09196634524a3d7269ac/opt_einsum-3.1.0.tar.gz (69kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 14.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/fd/1e86bc4837cc9a3a5faf3db9b1854aa04ad35b5f381f9648fbe81a6f94e4/google_pasta-0.1.8-py3-none-any.whl (57kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 13.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/site-packages (from tensorflow->fancyimpute) (0.33.6)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting dill>=0.3.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/11/345f3173809cea7f1a193bfbf02403fff250a3360e0e118a1630985e547d/dill-0.3.1.1.tar.gz (151kB)\n",
      "\u001b[K     |████████████████████████████████| 153kB 28.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting future\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "\u001b[K     |████████████████████████████████| 829kB 24.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/42/3aeda98f96e85fd26180534d36570e4d18108d62ae36f87694b476b83d6f/Werkzeug-0.16.0-py2.py3-none-any.whl (327kB)\n",
      "\u001b[K     |████████████████████████████████| 327kB 12.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow->fancyimpute) (41.6.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting markdown>=2.6.8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 21.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/cb/786dc53d93494784935a62947643b48250b84a882474e714f9af5e1a1928/google_auth-1.7.1-py2.py3-none-any.whl (74kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 16.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Collecting rsa<4.1,>=3.1.4\n",
      "  Downloading https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/50/bb4cefca37da63a0c52218ba2cb1b1c36110d84dcbae8aa48cd67c5e95c2/pyasn1_modules-0.2.7-py2.py3-none-any.whl (131kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 14.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<3.2,>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/2f/a6/30b0a0bef12283e83e58c1d6e7b5aabc7acfc4110df81a4471655d33e704/cachetools-3.1.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow->fancyimpute) (2.22.0)\n",
      "Collecting oauthlib>=3.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "\u001b[K     |████████████████████████████████| 153kB 19.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1>=0.1.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 14.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow->fancyimpute) (1.25.7)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow->fancyimpute) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow->fancyimpute) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow->fancyimpute) (2019.9.11)\n",
      "Building wheels for collected packages: fancyimpute, knnimpute, multiprocess, scs, ecos, pyyaml, wrapt, absl-py, gast, opt-einsum, termcolor, dill, future\n",
      "  Building wheel for fancyimpute (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fancyimpute: filename=fancyimpute-0.5.4-cp37-none-any.whl size=26387 sha256=cc33d7f002564c5868641302516872f4c42b0a6272218d3b19cfb1dc4c2f8e03\n",
      "  Stored in directory: /Users/jbiller/Library/Caches/pip/wheels/0e/65/31/fff6a8fa9d1df4c6204f5a9059340347d2085b971b67d3f0a0\n",
      "  Building wheel for knnimpute (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for knnimpute: filename=knnimpute-0.1.0-cp37-none-any.whl size=11355 sha256=f3888049f656c7c283e9c6d16648076232caa23c3a5221426e0436cc5b241ddf\n",
      "  Stored in directory: /Users/jbiller/Library/Caches/pip/wheels/a3/92/31/c3f8864714e9938396c3a68d8c542531f7e2d7862bb750b2e3\n",
      "  Building wheel for multiprocess (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for multiprocess: filename=multiprocess-0.70.9-cp37-none-any.whl size=108029 sha256=ce557c4d65dee98a9939b2480eaf5a8a3a2502e208f79779ac9d6c62e54bced7\n",
      "  Stored in directory: /Users/jbiller/Library/Caches/pip/wheels/96/20/ac/9f1d164f7d81787cd6f4401b1d05212807d021fbbbcc301b82\n",
      "  Building wheel for scs (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for scs: filename=scs-2.1.1.post2-cp37-cp37m-macosx_10_13_x86_64.whl size=117977 sha256=4da506f721ab6a323f30bdffb9f71925659a8c894bb5688f56aa0df0090cd9de\n",
      "  Stored in directory: /Users/jbiller/Library/Caches/pip/wheels/68/3f/24/e9c75d426f600634cdac68321184ba06fdc4ab2d189b5c4541\n",
      "  Building wheel for ecos (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ecos: filename=ecos-2.0.7.post1-cp37-cp37m-macosx_10_13_x86_64.whl size=83803 sha256=9b789ff9a93b7827aaa473e0c6ec1cb676a823bbecc472558ff92f7be6d38a6e\n",
      "  Stored in directory: /Users/jbiller/Library/Caches/pip/wheels/4b/7d/90/39ff7dca0e5c06740afc993ab4209a1719fc1d616daf7af040\n",
      "  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyyaml: filename=PyYAML-5.1.2-cp37-cp37m-macosx_10_13_x86_64.whl size=44111 sha256=b33e47e856387bbefefb8724ce09e965cd8ae5b9135e1ebfc7a0f995b72fd412\n",
      "  Stored in directory: /Users/jbiller/Library/Caches/pip/wheels/d9/45/dd/65f0b38450c47cf7e5312883deb97d065e030c5cca0a365030\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.11.2-cp37-cp37m-macosx_10_13_x86_64.whl size=32374 sha256=e449ea184b19e0824bb978181bd2e05bab76bdd7609c13e973f97a3b9f397755\n",
      "  Stored in directory: /Users/jbiller/Library/Caches/pip/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.8.1-cp37-none-any.whl size=121167 sha256=3742ad882f0c1a4d5e4d320dee94d7f2ed1575c3e3234858efa8095ec86c0cf8\n",
      "  Stored in directory: /Users/jbiller/Library/Caches/pip/wheels/a7/15/a0/0a0561549ad11cdc1bc8fa1191a353efd30facf6bfb507aefc\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=ba0f1d3f7cbed9245ecf9f6f556f752078e624db8066a7476788b23a16e79000\n",
      "  Stored in directory: /Users/jbiller/Library/Caches/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Building wheel for opt-einsum (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for opt-einsum: filename=opt_einsum-3.1.0-cp37-none-any.whl size=61682 sha256=4a035d16fa1861941a64c3c4c9fb87273bfe0173c1f655ad3eaf1e94bb9dd0ef\n",
      "  Stored in directory: /Users/jbiller/Library/Caches/pip/wheels/2c/b1/94/43d03e130b929aae7ba3f8d15cbd7bc0d1cb5bb38a5c721833\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-cp37-none-any.whl size=4832 sha256=b2d9d8c084aa96512fae4b025ff2a7b7a6078ee41a13927cbe0697eb64a0f15c\n",
      "  Stored in directory: /Users/jbiller/Library/Caches/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.1.1-cp37-none-any.whl size=78533 sha256=a33b1f1245fde27e132113f495bcf8af9a6733e5977ea5922f02efd92276aa30\n",
      "  Stored in directory: /Users/jbiller/Library/Caches/pip/wheels/59/b1/91/f02e76c732915c4015ab4010f3015469866c1eb9b14058d8e7\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491057 sha256=729f3d889f928d730c0a961dc8d0319a3626e78422e2b7ebfd638de58f1ce0e4\n",
      "  Stored in directory: /Users/jbiller/Library/Caches/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
      "Successfully built fancyimpute knnimpute multiprocess scs ecos pyyaml wrapt absl-py gast opt-einsum termcolor dill future\n",
      "Installing collected packages: knnimpute, dill, multiprocess, scs, ecos, future, osqp, cvxpy, h5py, keras-applications, keras-preprocessing, pyyaml, keras, astor, wrapt, werkzeug, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, protobuf, absl-py, grpcio, markdown, tensorboard, gast, tensorflow-estimator, opt-einsum, google-pasta, termcolor, tensorflow, fancyimpute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed absl-py-0.8.1 astor-0.8.0 cachetools-3.1.1 cvxpy-1.0.25 dill-0.3.1.1 ecos-2.0.7.post1 fancyimpute-0.5.4 future-0.18.2 gast-0.2.2 google-auth-1.7.1 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 keras-2.3.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 knnimpute-0.1.0 markdown-3.1.1 multiprocess-0.70.9 oauthlib-3.1.0 opt-einsum-3.1.0 osqp-0.6.1 protobuf-3.10.0 pyasn1-0.4.8 pyasn1-modules-0.2.7 pyyaml-5.1.2 requests-oauthlib-1.3.0 rsa-4.0 scs-2.1.1.post2 tensorboard-2.0.1 tensorflow-2.0.0 tensorflow-estimator-2.0.1 termcolor-1.1.0 werkzeug-0.16.0 wrapt-1.11.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fancyimpute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/b5/bab3477466a4d9e705d40829ac65683155e7977acbc07f05b06fabded1be/pandas-0.25.3-cp37-cp37m-macosx_10_9_x86_64.whl (10.2MB)\n",
      "\u001b[K     |████████████████████████████████| 10.2MB 2.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/Cellar/ipython/7.8.0/libexec/vendor/lib/python3.7/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/site-packages (from pandas) (1.17.4)\n",
      "Collecting pytz>=2017.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl (509kB)\n",
      "\u001b[K     |████████████████████████████████| 512kB 16.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/Cellar/ipython/7.8.0/libexec/vendor/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-0.25.3 pytz-2019.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/76/68bc3374ffa2d8d3dfd440fe94158fa8aa2628670fa38bdaf186c9af0d94/matplotlib-3.1.2-cp37-cp37m-macosx_10_9_x86_64.whl (13.2MB)\n",
      "\u001b[K     |████████████████████████████████| 13.2MB 2.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/0c/fc2e007d9a992d997f04a80125b0f183da7fb554f1de701bbb70a8e7d479/pyparsing-2.4.5-py2.py3-none-any.whl (67kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 15.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Collecting kiwisolver>=1.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/93/8bc9b52a8846be2b9572aa0a7c881930939b06e4abe1162da6a0430b794f/kiwisolver-1.1.0-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (113kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 11.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/Cellar/ipython/7.8.0/libexec/vendor/lib/python3.7/site-packages (from matplotlib) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/site-packages (from matplotlib) (1.17.4)\n",
      "Requirement already satisfied: six in /usr/local/Cellar/ipython/7.8.0/libexec/vendor/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (41.6.0)\n",
      "Installing collected packages: pyparsing, cycler, kiwisolver, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.1.0 matplotlib-3.1.2 pyparsing-2.4.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import Imputer  \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import VotingRegressor#create a dictionary of our models\n",
    "\n",
    "##from fancyimpute import MICE\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV #Importing RandomizedSearchCV for random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('https://grantmlong.com/data/SE_rents2018_train.csv', index_col=0)\n",
    "test_df = pd.read_csv('https://grantmlong.com/data/SE_rents2018_test1.csv', index_col=0)\n",
    "submit1_df = pd.read_csv('https://grantmlong.com/data/SE_rents2018_test2.csv', index_col=0)\n",
    "submit2_df = pd.read_csv('https://grantmlong.com/data/SE_rents2018_test3.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract usable features\n",
    "feature_cols = [\n",
    "    'bedrooms', 'bathrooms', 'floor_count', 'size_sqft',\n",
    "    'has_washer_dryer', 'has_roofdeck', 'has_doorman', 'has_dishwasher', 'has_concierge', 'has_elevator'\n",
    "]\n",
    "train_features = train_df[feature_cols] \n",
    "\n",
    "# impute missing values with imputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imp.fit(train_features)\n",
    "imp.transform(train_features)\n",
    "#LOOK INTO IMPUTING USING MICE-- see fancyimpute\n",
    "\n",
    "# construct target vector\n",
    "train_target = train_df['rent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit models for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atsui\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingRegressor(estimators=[('knn',\n",
       "                             GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                                          estimator=KNeighborsRegressor(algorithm='auto',\n",
       "                                                                        leaf_size=30,\n",
       "                                                                        metric='minkowski',\n",
       "                                                                        metric_params=None,\n",
       "                                                                        n_jobs=None,\n",
       "                                                                        n_neighbors=5,\n",
       "                                                                        p=2,\n",
       "                                                                        weights='uniform'),\n",
       "                                          iid='warn', n_jobs=None,\n",
       "                                          param_grid={'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24...\n",
       "                                                                                   min_weight_fraction_leaf=0.0,\n",
       "                                                                                   presort=False,\n",
       "                                                                                   random_state=1,\n",
       "                                                                                   splitter='best'),\n",
       "                                              bootstrap=True,\n",
       "                                              bootstrap_features=False,\n",
       "                                              max_features=1.0, max_samples=1.0,\n",
       "                                              n_estimators=10, n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False)),\n",
       "                            ('ab',\n",
       "                             AdaBoostRegressor(base_estimator=None,\n",
       "                                               learning_rate=1.0, loss='linear',\n",
       "                                               n_estimators=50,\n",
       "                                               random_state=None))],\n",
       "                n_jobs=None, weights=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg = LinearRegression()\n",
    "lreg.fit(train_features, train_target)\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(train_features, train_target)\n",
    "\n",
    "br = BaggingRegressor(tree.DecisionTreeRegressor(random_state=1))\n",
    "br.fit(train_features, train_target)\n",
    "\n",
    "gb = GradientBoostingRegressor()\n",
    "gb.fit(train_features, train_target)\n",
    "\n",
    "ab = AdaBoostRegressor()\n",
    "ab.fit(train_features, train_target)\n",
    "\n",
    "knn = KNeighborsRegressor()#create a dictionary of all values we want to test for n_neighbors\n",
    "params_knn = {'n_neighbors': np.arange(1, 25)}#use gridsearch to test all values for n_neighbors\n",
    "knn_gs = GridSearchCV(knn, params_knn, cv=5)#fit model to training data\n",
    "knn_gs.fit(train_features, train_target)\n",
    "\n",
    "estimators=[('knn', knn_gs), ('rf', rf), ('lreg', lreg),('br',br),('ab',ab)]\n",
    "voter = VotingRegressor(estimators)\n",
    "voter.fit(train_features, train_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n"
     ]
    }
   ],
   "source": [
    "#pprint(rf.get_params()) #Checking hyper parameters of rf\n",
    "\n",
    "#Creating our random grid\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "pprint(random_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  30 | elapsed:   34.1s remaining:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   57.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                   estimator=RandomForestRegressor(bootstrap=True,\n",
       "                                                   criterion='mse',\n",
       "                                                   max_depth=None,\n",
       "                                                   max_features='auto',\n",
       "                                                   max_leaf_nodes=None,\n",
       "                                                   min_impurity_decrease=0.0,\n",
       "                                                   min_impurity_split=None,\n",
       "                                                   min_samples_leaf=1,\n",
       "                                                   min_samples_split=2,\n",
       "                                                   min_weight_fraction_leaf=0.0,\n",
       "                                                   n_estimators='warn',\n",
       "                                                   n_jobs=None, oob_score=False,\n",
       "                                                   random_sta...\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rrf = RandomForestRegressor() #Base RFR for use in our RandomizedSCV\n",
    "rf_random = RandomizedSearchCV(estimator = rrf, \n",
    "                               param_distributions = random_grid, \n",
    "                               n_iter = 10, cv = 3, verbose = 2,\n",
    "                               random_state = 42, n_jobs = -1)\n",
    "\n",
    "rf_random.fit(train_features, train_target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 1400,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 30,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_   #Looking at best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf_random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4a219e0fc548>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbest_random\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m   \u001b[1;31m#Setting new RSCV using best parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbest_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rf_random' is not defined"
     ]
    }
   ],
   "source": [
    "best_random = rf_random.best_estimator_   #Setting new RSCV using best parameters\n",
    "best_random.fit(train_features, train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict and Measure Using Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE_rf 2495488.91707093\n",
      "MSE_gb 2492725.897541145\n",
      "MSE_ab 13656505.341842923\n",
      "MSE_br 2286546.0623431825\n",
      "MSE_lreg 3673665.713656922\n",
      "MSE_knn 3089202.84534\n",
      "MSE_voter 2779327.7179520084\n"
     ]
    }
   ],
   "source": [
    "test_features = test_df[feature_cols] \n",
    "\n",
    "# impute missing values with medians\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imp.fit(test_features)\n",
    "imp.transform(test_features)\n",
    "\n",
    "# construct predictions \n",
    "test_df['predicted'] = rf.predict(test_features)\n",
    "print(\"MSE_rf\", mean_squared_error(test_df['rent'], test_df['predicted']))\n",
    "test_df = test_df.drop('predicted', 1)\n",
    "\n",
    "test_df['predicted'] = gb.predict(test_features)\n",
    "print(\"MSE_gb\", mean_squared_error(test_df['rent'], test_df['predicted']))\n",
    "test_df = test_df.drop('predicted', 1)\n",
    "\n",
    "test_df['predicted'] = ab.predict(test_features)\n",
    "print(\"MSE_ab\", mean_squared_error(test_df['rent'], test_df['predicted']))\n",
    "test_df = test_df.drop('predicted', 1)\n",
    "\n",
    "test_df['predicted'] = br.predict(test_features)\n",
    "print(\"MSE_br\", mean_squared_error(test_df['rent'], test_df['predicted']))\n",
    "test_df = test_df.drop('predicted', 1)\n",
    "\n",
    "test_df['predicted'] = lreg.predict(test_features)\n",
    "print(\"MSE_lreg\", mean_squared_error(test_df['rent'], test_df['predicted']))\n",
    "test_df = test_df.drop('predicted', 1)\n",
    "\n",
    "test_df['predicted'] = knn_gs.predict(test_features)\n",
    "print(\"MSE_knn\", mean_squared_error(test_df['rent'], test_df['predicted']))\n",
    "test_df = test_df.drop('predicted', 1)\n",
    "\n",
    "test_df['predicted'] = voter.predict(test_features)\n",
    "print(\"MSE_voter\", mean_squared_error(test_df['rent'], test_df['predicted']))\n",
    "test_df = test_df.drop('predicted', 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict and Measure Using Test 1 (Randomized Search CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE_RSCV 2271917.461044621\n",
      "MSE_Best_RSCV 2271917.461044621\n"
     ]
    }
   ],
   "source": [
    "test_df['predicted'] = rf_random.predict(test_features)   #Testing randomized search CV\n",
    "print(\"MSE_RSCV\", mean_squared_error(test_df['rent'], test_df['predicted']))\n",
    "test_df = test_df.drop('predicted', 1)\n",
    "\n",
    "test_df['predicted'] = best_random.predict(test_features)   #Testing randomized search CV\n",
    "print(\"MSE_Best_RSCV\", mean_squared_error(test_df['rent'], test_df['predicted']))\n",
    "test_df = test_df.drop('predicted', 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Data, Predict Values for Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingRegressor(estimators=[('knn',\n",
       "                             GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                                          estimator=KNeighborsRegressor(algorithm='auto',\n",
       "                                                                        leaf_size=30,\n",
       "                                                                        metric='minkowski',\n",
       "                                                                        metric_params=None,\n",
       "                                                                        n_jobs=None,\n",
       "                                                                        n_neighbors=5,\n",
       "                                                                        p=2,\n",
       "                                                                        weights='uniform'),\n",
       "                                          iid='warn', n_jobs=None,\n",
       "                                          param_grid={'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24...\n",
       "                                                                                   min_weight_fraction_leaf=0.0,\n",
       "                                                                                   presort=False,\n",
       "                                                                                   random_state=1,\n",
       "                                                                                   splitter='best'),\n",
       "                                              bootstrap=True,\n",
       "                                              bootstrap_features=False,\n",
       "                                              max_features=1.0, max_samples=1.0,\n",
       "                                              n_estimators=10, n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False)),\n",
       "                            ('ab',\n",
       "                             AdaBoostRegressor(base_estimator=None,\n",
       "                                               learning_rate=1.0, loss='linear',\n",
       "                                               n_estimators=50,\n",
       "                                               random_state=None))],\n",
       "                n_jobs=None, weights=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df = train_df.append(test_df, sort=False)\n",
    "\n",
    "# impute missing values with imputer\n",
    "#imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "#imp.fit(master_features)\n",
    "#imp.transform(master_features)\n",
    "\n",
    "master_features = master_df[feature_cols].fillna(master_df[feature_cols].median(), axis=0)\n",
    "\n",
    "master_target = master_df['rent']\n",
    "\n",
    "rf.fit(master_features, master_target)\n",
    "gb.fit(master_features, master_target)\n",
    "ab.fit(master_features, master_target)\n",
    "br.fit(master_features, master_target)\n",
    "lreg.fit(master_features, master_target)\n",
    "\n",
    "params_knn = {'n_neighbors': np.arange(1, 25)}#use gridsearch to test all values for n_neighbors\n",
    "knn_gs = GridSearchCV(knn, params_knn, cv=5)#fit model to training data\n",
    "knn_gs.fit(master_features, master_target)\n",
    "\n",
    "estimators=[('knn', knn_gs), ('rf', rf), ('lreg', lreg),('br',br),('ab',ab)]\n",
    "voter = VotingRegressor(estimators)\n",
    "voter.fit(master_features, master_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomized Search CV for test 2 (Without best parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  30 | elapsed:   39.7s remaining:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=30,\n",
       "                      max_features='sqrt', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=5,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=1400,\n",
       "                      n_jobs=None, oob_score=False, random_state=None,\n",
       "                      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.fit(master_features, master_target)\n",
    "best_random.fit(master_features, master_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomized Search CV (test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVE_rf:  5158562.8688808335\n",
      "MVE_best_rf:  4959437.607092596\n"
     ]
    }
   ],
   "source": [
    "submit2_features = submit2_df[feature_cols].fillna(master_df[feature_cols].median(), axis=0)\n",
    "submit2_df['predictions'] = rf_random.predict(submit2_features)\n",
    "submit2_df['predictions'].to_csv('submission_rf.csv', header=True)\n",
    "\n",
    "submit2_df['fake_rent'] = np.ones(submit2_df['predictions'].shape) * master_target.median()\n",
    "print(\"MVE_rf: \", mean_squared_error(submit2_df['predictions'], submit2_df['fake_rent']))\n",
    "\n",
    "submit2_features = submit2_df[feature_cols].fillna(master_df[feature_cols].median(), axis=0)\n",
    "submit2_df['predictions'] = best_random.predict(submit2_features)\n",
    "submit2_df['predictions'].to_csv('submission_rf.csv', header=True)\n",
    "\n",
    "submit2_df['fake_rent'] = np.ones(submit2_df['predictions'].shape) * master_target.median()\n",
    "print(\"MVE_best_rf: \", mean_squared_error(submit2_df['predictions'], submit2_df['fake_rent']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor (test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVE_rf:  5466196.053009413\n"
     ]
    }
   ],
   "source": [
    "submit2_features = submit2_df[feature_cols].fillna(master_df[feature_cols].median(), axis=0)\n",
    "submit2_df['predictions'] = rf.predict(submit2_features)\n",
    "submit2_df['predictions'].to_csv('submission_rf.csv', header=True)\n",
    "\n",
    "submit2_df['fake_rent'] = np.ones(submit2_df['predictions'].shape) * master_target.median()\n",
    "print(\"MVE_rf: \", mean_squared_error(submit2_df['predictions'], submit2_df['fake_rent']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradientBoosting Regressor (test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVE_gb:  5466196.053009413\n"
     ]
    }
   ],
   "source": [
    "submit2_features = submit2_df[feature_cols].fillna(master_df[feature_cols].median(), axis=0)\n",
    "submit2_df['predictions'] = rf.predict(submit2_features)\n",
    "submit2_df['predictions'].to_csv('sample_submission2.csv', header=True)\n",
    "\n",
    "submit2_df['fake_rent'] = np.ones(submit2_df['predictions'].shape) * master_target.median()\n",
    "print(\"MVE_gb: \", mean_squared_error(submit2_df['predictions'], submit2_df['fake_rent']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ABoost Regressor (test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVE_ab:  8249979.111599247\n"
     ]
    }
   ],
   "source": [
    "submit2_features = submit2_df[feature_cols].fillna(master_df[feature_cols].median(), axis=0)\n",
    "submit2_df['predictions'] = ab.predict(submit2_features)\n",
    "submit2_df['predictions'].to_csv('submission_ab.csv', header=True)\n",
    "\n",
    "submit2_df['fake_rent'] = np.ones(submit2_df['predictions'].shape) * master_target.median()\n",
    "print(\"MVE_ab: \", mean_squared_error(submit2_df['predictions'], submit2_df['fake_rent']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagged Regressor (test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVE_br:  5528268.445643128\n"
     ]
    }
   ],
   "source": [
    "submit2_features = submit2_df[feature_cols].fillna(master_df[feature_cols].median(), axis=0)\n",
    "submit2_df['predictions'] = br.predict(submit2_features)\n",
    "submit2_df['predictions'].to_csv('submission_br.csv', header=True)\n",
    "\n",
    "submit2_df['fake_rent'] = np.ones(submit2_df['predictions'].shape) * master_target.median()\n",
    "print(\"MVE_br: \", mean_squared_error(submit2_df['predictions'], submit2_df['fake_rent']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression (test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVE_lreg:  4180217.401876736\n"
     ]
    }
   ],
   "source": [
    "submit2_features = submit2_df[feature_cols].fillna(master_df[feature_cols].median(), axis=0)\n",
    "submit2_df['predictions'] = lreg.predict(submit2_features)\n",
    "submit2_df['predictions'].to_csv('submission_lreg.csv', header=True)\n",
    "\n",
    "submit2_df['fake_rent'] = np.ones(submit2_df['predictions'].shape) * master_target.median()\n",
    "print(\"MVE_lreg: \", mean_squared_error(submit2_df['predictions'], submit2_df['fake_rent']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Regressor (test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVE_knn:  4588720.466078125\n"
     ]
    }
   ],
   "source": [
    "submit2_features = submit2_df[feature_cols].fillna(master_df[feature_cols].median(), axis=0)\n",
    "submit2_df['predictions'] = knn_gs.predict(submit2_features)\n",
    "submit2_df['predictions'].to_csv('submission_knn.csv', header=True)\n",
    "\n",
    "submit2_df['fake_rent'] = np.ones(submit2_df['predictions'].shape) * master_target.median()\n",
    "print(\"MVE_knn: \", mean_squared_error(submit2_df['predictions'], submit2_df['fake_rent']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Regressor (test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVE_vote:  4703455.464516889\n"
     ]
    }
   ],
   "source": [
    "submit2_features = submit2_df[feature_cols].fillna(master_df[feature_cols].median(), axis=0)\n",
    "submit2_df['predictions'] = voter.predict(submit2_features)\n",
    "submit2_df['predictions'].to_csv('submission_vote.csv', header=True)\n",
    "\n",
    "submit2_df['fake_rent'] = np.ones(submit2_df['predictions'].shape) * master_target.median()\n",
    "print(\"MVE_vote: \", mean_squared_error(submit2_df['predictions'], submit2_df['fake_rent']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
